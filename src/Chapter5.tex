\chapter{Conditional Probability}
Often we need to calculate the probability of some event~$A$ occurring while knowing that some other event~$B$ has already occurred. We call this the conditional probability of $A$ \textbf{given} $B$ and denote it by $P(A \given B)$.
\par\smallskip
The conditional probability of event~$A$, given event~$B$, is
\[
    P(A \given B) = \frac{P(A \intersect B)}{P(B)}\for P(B) > 0
\]
\section{Theorems and Rules for Conditional Probability}
\begin{theorem}
For any two events~$A$ and~$B$ defined on the same sample space, with $P(A) > 0$ and $P(B) > 0$, events~$A$ and~$B$ are independent if and only if $P(A \given B) = P(A)$ or $P(B \given A) = P(B)$.
\end{theorem}
\begin{proof}
\begin{align*}
    P(A \given B) &= \frac{P(A \intersect B)}{P(B)} \\
    \ifo 
    P(A \intersect B) &= P(A \given B)P(B)
\end{align*}
and by definition of independence,~$A$ and~$B$ are independent if and only if $P(A \intersect B) = P(A)P(B)$ which is true if and only if $P(A \given B) = P(A)$. Without loss of generality we can swap events~$A$ and~$B$ and arrive at the conclusion.
\end{proof}
\subsection*{Product Rules}
\begin{theorem}
Let $A, B, C$ and~$D$ be events on a sample space, with $P(A), P(B), P(C), P(D) > 0$. We have
\begin{align*}
    P(A \intersect B) &= P(A)P(B \given A) \\
    P(A \intersect B \intersect C) &= P(A)P(B \given A)P(C \given A \intersect B) \\
    P(A \intersect B \intersect C \intersect D) &=
    P(A)P(B \given A)P(C \given A \intersect B)P(D \given A \intersect B \intersect C) \\
\end{align*}
and so on$\ldots$
\end{theorem}
\begin{proof}
The first statement come directly from the definition of conditional probability
\[
    P(A)P(B \given A) = P(A) \frac{P(A \intersect B)}{P(A)} = P(A \intersect B)
\]
For the second we have
\begin{align*}
    P(A)P(B \given A)P(C \given A \intersect B)
    &= P(A \intersect B)P(C \given A \intersect B)
    &\by{the first statement} \\
    &= P(A \intersect B) \frac{P(A \intersect B \intersect C)}{P(A \intersect B)}
    &\by{definition of conditional probability} \\
    &= P(A \intersect B \intersect C)
\end{align*}
and so on$\ldots$
\end{proof}
\subsection*{Law of Total Probability}
\begin{theorem}
Let $A_1, A_2, A_3,\ldots,A_k$ be mutually exclusive events on a sample space and let $B$ be an event on the same sample space. We have
\[
    P(B) = P(B \intersect A_1) + P(B \intersect A_2) + P(B \intersect A_3) + \cdots + P(B \intersect A_k)
    = \sum_{i = 1}^{k} P(A_i)P(B \given A_i)
\]
\end{theorem}
\begin{proof}
Note that the events $A_i \intersect B$ for $1 \leq i \leq k$ are all mutually exclusive events since $A_i$'s are mutually exclusive. Thus, the union of the $A_i \intersect B$'s is $B$, that is 
\[
    (A_1 \intersect B) \union (A_2 \intersect B) \union (A_3 \intersect B) \union \cdots \union (A_k \intersect B) = B
\]
So by Theorem 4.5.1, we have
\[
    P(B) = P(A_1 \intersect B) + P(A_2 \intersect B) + P(A_3 \intersect B) + \cdots + P(A_k \intersect B)
\]
and by Theorem 5.1.2 (Product Rule), we have
\[
    P(B) = P(A_1)P(B \given A_1) + P(A_2)P(B \given A_2) + P(A_3)P(B \given A_3) + \cdots + P(A_k)P(B \given A_k) = \sum_{i = 1}^{k} P(A_i)P(B \given A_i)
\]
as required.
\end{proof}
\subsection*{Bayes' Theorem}
\begin{theorem}
Let $A$ and~$B$ be events on a sample space, with $P(B) > 0$. We have
\[
    P(A \given B) = \frac{P(B \given A)P(A)}{P(B)}
    = \frac{P(B \given A)P(A)}{P(B \given \comp{A})P(\comp{A}) + P(B \given A)P(A)}
\]
\end{theorem}
\begin{proof}
\begin{align*}
    P(A \given B)
    &= \frac{P(A \intersect B)}{P(B)} 
    = \frac{P(B \given A)P(A)}{P(B)}\by{Theorem 4.7.2 (Product Rule)} \\
    &= \frac{P(B \given A)P(A)}{P(A \intersect B) + P(\comp{A} \intersect B)}
    \by{Theorem 4.7.3 (Law of Total Probability)} \\
    &= \frac{P(B \given A)P(A)}{P(B \given \comp{A})P(\comp{A}) + P(B \given A)P(A)}\by{Theorem 4.7.2 (Product Rule)}
\end{align*}
\end{proof}
\begin{info}
Bayes' Theorem allows us to find the conditional probability of some event~$A$ given~$B$, in terms of the probability of~$B$ given~$A$. It allows us calculate conditional probabilities using the reversed order of conditioning.
\end{info}
\section{Tree Diagrams}
Tree diagrams are a technique that we can use to keep track of conditional probabilities. We start from a single node and draw new branches to separate nodes for each event. Each node represents the event occurring. On each branch we write the probability of event it leads to occurring. To find the probability of any node we multiply the probabilities of the branches leading to the node.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=5mm},
]
%Nodes
\node[roundnode]        (root)                              {};
\node[roundnode]        (A2)        [right=3cm of root]                 {$\scriptstyle A_2$};
\node[roundnode]        (A1)        [above=1.5cm of A2]                 {$\scriptstyle A_1$};
\node[roundnode]        (A3)        [below=1.5cm of A2]                 {$\scriptstyle A_3$};
\node[roundnode]        (A1B)       [above right=0.25cm and 3cm of A1]   {$\scriptstyle B$};
\node[roundnode]        (A1compB)   [below=0.5cm of A1B]                {$\scriptstyle\comp{B}$};
\node[roundnode]        (A2B)       [above right=0.25cm and 3cm of A2]   {$\scriptstyle B$};
\node[roundnode]        (A2compB)   [below=0.5cm of A2B]                {$\scriptstyle\comp{B}$};
\node[roundnode]        (A3B)       [above right=0.25cm and 3cm of A3]   {$\scriptstyle B$};
\node[roundnode]        (A3compB)   [below=0.5cm of A3B]                {$\scriptstyle\comp{B}$};
 
%Lines
\draw[->] (root.east) -- (A1.west) node[midway, sloped, above] {0.5};
\draw[->] (root.east) -- (A2.west) node[midway, sloped, above] {0.3};
\draw[->] (root.east) -- (A3.west) node[midway, sloped, above] {0.2};
\draw[->] (A1.east) -- (A1B.west) node[midway, sloped, above] {0.7};
\draw[->] (A1.east) -- (A1compB.west) node[midway, sloped, above] {0.3};
\draw[->] (A2.east) -- (A2B.west) node[midway, sloped, above] {0.9};
\draw[->] (A2.east) -- (A2compB.west) node[midway, sloped, above] {0.1};
\draw[->] (A3.east) -- (A3B.west) node[midway, sloped, above] {0.4};
\draw[->] (A3.east) -- (A3compB.west) node[midway, sloped, above] {0.6};

%Probabilities
\node [right=3mm of A1B]
    {$P(A_1 \intersect B) = P(B \given A_1)P(A_1) = 0.5 \times 0.7$};
\node [right=3mm of A1compB]
    {$P(A_1 \intersect \comp{B}) = P(\comp{B} \given A_1)P(A_1) = 0.5 \times 0.3$};
\node [right=3mm of A2B]
    {$P(A_2 \intersect B) = P(B \given A_2)P(A_2) = 0.3 \times 0.9$};
\node [right=3mm of A2compB]
    {$P(A_2 \intersect \comp{B}) = P(\comp{B} \given A_2)P(A_2) = 0.3 \times 0.1$};
\node [right=3mm of A3B]
    {$P(A_3 \intersect B) = P(B \given A_3)P(A_3) = 0.2 \times 0.4$};
\node [right=3mm of A3compB]
    {$P(A_3 \intersect \comp{B}) = P(\comp{B} \given A_3)P(A_3) = 0.2 \times 0.6$};
\end{tikzpicture}
\caption{Tree diagram} \label{fig:Tree diagram}
\end{figure}
\begin{info}
The probability of all the the branches leading outward from each node must sum to~ 1 since at least one outcome must occur.
\end{info}
